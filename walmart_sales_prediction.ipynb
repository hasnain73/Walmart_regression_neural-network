{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üè™ Walmart Weekly Sales Prediction - Neural Network\n",
                "\n",
                "**Goal:** Build a simple Neural Network for regression to predict weekly sales.\n",
                "\n",
                "**Dataset:** [Kaggle - Walmart Sales](https://www.kaggle.com/datasets/mikhail1681/walmart-sales)\n",
                "\n",
                "---\n",
                "\n",
                "## üìå Project Overview\n",
                "\n",
                "- **Model:** Simple Neural Network (64‚Üí32‚Üí1 neurons)\n",
                "- **Framework:** TensorFlow/Keras\n",
                "- **Training:** 20 epochs, batch_size=32\n",
                "- **Loss:** MSE (Mean Squared Error)\n",
                "- **Deployment:** Hugging Face Spaces with Gradio\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ Setup Kaggle API (Upload kaggle.json)\n",
                "\n",
                "**Instructions:**\n",
                "1. Go to your Kaggle account settings\n",
                "2. Create new API token\n",
                "3. Download `kaggle.json`\n",
                "4. Upload it to Colab using the file upload below"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Upload kaggle.json file\n",
                "from google.colab import files\n",
                "uploaded = files.upload()\n",
                "\n",
                "# Setup Kaggle credentials\n",
                "!mkdir -p ~/.kaggle\n",
                "!cp kaggle.json ~/.kaggle/\n",
                "!chmod 600 ~/.kaggle/kaggle.json\n",
                "\n",
                "print(\"‚úì Kaggle API configured!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ Install Required Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%capture\n",
                "# Install libraries (suppress output)\n",
                "!pip install pandas numpy scikit-learn tensorflow matplotlib joblib"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3Ô∏è‚É£ Download Dataset from Kaggle"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Download dataset\n",
                "!kaggle datasets download -d mikhail1681/walmart-sales\n",
                "\n",
                "# Unzip\n",
                "!unzip -o walmart-sales.zip\n",
                "\n",
                "print(\"\\n‚úì Dataset downloaded and extracted!\")\n",
                "!ls -lh *.csv"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4Ô∏è‚É£ Import Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
                "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "from tensorflow.keras import layers\n",
                "import joblib\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Set random seeds\n",
                "np.random.seed(42)\n",
                "tf.random.set_seed(42)\n",
                "\n",
                "print(f\"TensorFlow Version: {tf.__version__}\")\n",
                "print(\"‚úì Libraries imported!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5Ô∏è‚É£ Load and Explore Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load dataset\n",
                "df = pd.read_csv('Walmart.csv')\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"DATASET OVERVIEW\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Shape: {df.shape}\")\n",
                "print(f\"\\nColumns: {list(df.columns)}\")\n",
                "print(f\"\\nFirst 5 rows:\")\n",
                "display(df.head())\n",
                "\n",
                "print(f\"\\nData Types:\")\n",
                "display(df.dtypes)\n",
                "\n",
                "print(f\"\\nMissing Values:\")\n",
                "display(df.isnull().sum())\n",
                "\n",
                "print(f\"\\nBasic Statistics:\")\n",
                "display(df.describe())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6Ô∏è‚É£ Data Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*60)\n",
                "print(\"DATA PREPROCESSING\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Handle missing values - fill with median\n",
                "print(\"\\n[1/4] Handling missing values...\")\n",
                "df.fillna(df.median(numeric_only=True), inplace=True)\n",
                "print(\"‚úì Missing values handled\")\n",
                "\n",
                "# Drop Date column (not useful for simple model)\n",
                "print(\"\\n[2/4] Dropping unnecessary columns...\")\n",
                "if 'Date' in df.columns:\n",
                "    df = df.drop(columns=['Date'])\n",
                "    print(\"‚úì Dropped 'Date' column\")\n",
                "\n",
                "# Separate features and target\n",
                "print(\"\\n[3/4] Separating features and target...\")\n",
                "target_col = 'Weekly_Sales'\n",
                "X = df.drop(columns=[target_col])\n",
                "y = df[target_col]\n",
                "print(f\"‚úì Features shape: {X.shape}\")\n",
                "print(f\"‚úì Target shape: {y.shape}\")\n",
                "\n",
                "# Encode categorical features\n",
                "print(\"\\n[4/4] Encoding categorical features...\")\n",
                "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
                "print(f\"Categorical columns: {categorical_cols}\")\n",
                "\n",
                "label_encoders = {}\n",
                "for col in categorical_cols:\n",
                "    le = LabelEncoder()\n",
                "    X[col] = le.fit_transform(X[col].astype(str))\n",
                "    label_encoders[col] = le\n",
                "    \n",
                "print(\"‚úì Categorical features encoded\")\n",
                "print(\"\\nFinal Features:\")\n",
                "display(X.head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7Ô∏è‚É£ Train-Test Split & Scaling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*60)\n",
                "print(\"TRAIN-TEST SPLIT & SCALING\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Split data (80% train, 20% test)\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, test_size=0.2, random_state=42\n",
                ")\n",
                "\n",
                "print(f\"\\nTraining set: {X_train.shape[0]} samples\")\n",
                "print(f\"Test set: {X_test.shape[0]} samples\")\n",
                "\n",
                "# Scale features using StandardScaler\n",
                "scaler = StandardScaler()\n",
                "X_train_scaled = scaler.fit_transform(X_train)\n",
                "X_test_scaled = scaler.transform(X_test)\n",
                "\n",
                "print(\"\\n‚úì Features scaled using StandardScaler\")\n",
                "print(f\"Feature shape: {X_train_scaled.shape[1]} features\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8Ô∏è‚É£ Build Neural Network Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*60)\n",
                "print(\"BUILDING NEURAL NETWORK\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Define model architecture\n",
                "model = keras.Sequential([\n",
                "    layers.Input(shape=(X_train_scaled.shape[1],)),\n",
                "    layers.Dense(64, activation='relu', name='hidden_layer_1'),\n",
                "    layers.Dense(32, activation='relu', name='hidden_layer_2'),\n",
                "    layers.Dense(1, activation='linear', name='output_layer')\n",
                "], name='Walmart_Sales_NN')\n",
                "\n",
                "# Compile model\n",
                "model.compile(\n",
                "    optimizer='adam',\n",
                "    loss='mse',\n",
                "    metrics=['mae']\n",
                ")\n",
                "\n",
                "print(\"\\nModel Architecture:\")\n",
                "model.summary()\n",
                "print(\"\\n‚úì Model built and compiled!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9Ô∏è‚É£ Train Model (20 Epochs)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*60)\n",
                "print(\"TRAINING MODEL\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Train model\n",
                "history = model.fit(\n",
                "    X_train_scaled, y_train,\n",
                "    epochs=20,\n",
                "    batch_size=32,\n",
                "    validation_split=0.2,\n",
                "    verbose=1\n",
                ")\n",
                "\n",
                "print(\"\\n‚úì Training complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üîü Evaluate Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*60)\n",
                "print(\"MODEL EVALUATION\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Make predictions\n",
                "y_pred = model.predict(X_test_scaled).flatten()\n",
                "\n",
                "# Calculate metrics\n",
                "mse = mean_squared_error(y_test, y_pred)\n",
                "mae = mean_absolute_error(y_test, y_pred)\n",
                "rmse = np.sqrt(mse)\n",
                "r2 = r2_score(y_test, y_pred)\n",
                "\n",
                "print(\"\\nTest Set Metrics:\")\n",
                "print(\"-\" * 40)\n",
                "print(f\"MSE (Mean Squared Error):  ${mse:,.2f}\")\n",
                "print(f\"MAE (Mean Absolute Error): ${mae:,.2f}\")\n",
                "print(f\"RMSE (Root MSE):           ${rmse:,.2f}\")\n",
                "print(f\"R¬≤ Score:                  {r2:.4f}\")\n",
                "print(\"-\" * 40)\n",
                "\n",
                "if r2 > 0.7:\n",
                "    print(\"\\n‚úÖ Model performance: GOOD\")\n",
                "elif r2 > 0.5:\n",
                "    print(\"\\n‚ö†Ô∏è Model performance: MODERATE\")\n",
                "else:\n",
                "    print(\"\\n‚ùå Model performance: NEEDS IMPROVEMENT\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£1Ô∏è‚É£ Visualizations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot training history and predictions\n",
                "plt.figure(figsize=(14, 5))\n",
                "\n",
                "# Plot 1: Training vs Validation Loss\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.plot(history.history['loss'], label='Training Loss', linewidth=2, color='blue')\n",
                "plt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2, color='orange')\n",
                "plt.title('Training vs Validation Loss', fontsize=14, fontweight='bold')\n",
                "plt.xlabel('Epoch', fontsize=12)\n",
                "plt.ylabel('MSE Loss', fontsize=12)\n",
                "plt.legend(fontsize=10)\n",
                "plt.grid(True, alpha=0.3)\n",
                "\n",
                "# Plot 2: Predicted vs Actual\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.scatter(y_test, y_pred, alpha=0.5, edgecolors='black', linewidth=0.5, s=50)\n",
                "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
                "         'r--', linewidth=2, label='Perfect Prediction')\n",
                "plt.title('Predicted vs Actual Sales', fontsize=14, fontweight='bold')\n",
                "plt.xlabel('Actual Weekly Sales ($)', fontsize=12)\n",
                "plt.ylabel('Predicted Weekly Sales ($)', fontsize=12)\n",
                "plt.legend(fontsize=10)\n",
                "plt.grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('training_results.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(\"‚úì Visualizations saved as 'training_results.png'\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£2Ô∏è‚É£ Save Model & Preprocessors"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*60)\n",
                "print(\"SAVING MODEL & PREPROCESSORS\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Save model\n",
                "model.save('walmart_sales_model.h5')\n",
                "print(\"\\n‚úì Model saved: walmart_sales_model.h5\")\n",
                "\n",
                "# Save scaler\n",
                "joblib.dump(scaler, 'scaler.pkl')\n",
                "print(\"‚úì Scaler saved: scaler.pkl\")\n",
                "\n",
                "# Save feature names\n",
                "feature_names = X.columns.tolist()\n",
                "joblib.dump(feature_names, 'feature_names.pkl')\n",
                "print(\"‚úì Feature names saved: feature_names.pkl\")\n",
                "\n",
                "# Save label encoders if any\n",
                "if label_encoders:\n",
                "    joblib.dump(label_encoders, 'label_encoders.pkl')\n",
                "    print(\"‚úì Label encoders saved: label_encoders.pkl\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"‚úÖ ALL FILES SAVED SUCCESSFULLY!\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£3Ô∏è‚É£ Download Files for Deployment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Download all necessary files\n",
                "from google.colab import files\n",
                "\n",
                "print(\"Preparing files for download...\\n\")\n",
                "\n",
                "files.download('walmart_sales_model.h5')\n",
                "files.download('scaler.pkl')\n",
                "files.download('feature_names.pkl')\n",
                "files.download('training_results.png')\n",
                "\n",
                "if label_encoders:\n",
                "    files.download('label_encoders.pkl')\n",
                "\n",
                "print(\"\\n‚úì Files downloaded! Ready for Hugging Face deployment.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## ü§ó Hugging Face Deployment Instructions\n",
                "\n",
                "### Step 1: Create Hugging Face Account\n",
                "1. Go to [huggingface.co](https://huggingface.co)\n",
                "2. Sign up for free account\n",
                "\n",
                "### Step 2: Create New Space\n",
                "1. Click **\"New Space\"**\n",
                "2. Name: `walmart-sales-predictor`\n",
                "3. SDK: **Gradio**\n",
                "4. Hardware: CPU (free)\n",
                "\n",
                "### Step 3: Upload Files\n",
                "Upload these files to your Space:\n",
                "- `walmart_sales_model.h5` (rename to `model.h5`)\n",
                "- `scaler.pkl`\n",
                "- `feature_names.pkl`\n",
                "- `app.py` (create from code below)\n",
                "- `requirements.txt` (create from code below)\n",
                "\n",
                "### Step 4: Create app.py\n",
                "See the next cell for the complete app.py code.\n",
                "\n",
                "### Step 5: Create requirements.txt\n",
                "```\n",
                "tensorflow==2.13.0\n",
                "gradio==4.19.2\n",
                "joblib==1.3.2\n",
                "numpy==1.24.3\n",
                "```\n",
                "\n",
                "### Step 6: Deploy!\n",
                "Your app will be live at: `https://huggingface.co/spaces/YOUR_USERNAME/walmart-sales-predictor`\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìù app.py Code for Hugging Face"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# This is the app.py code - copy this to your Hugging Face Space\n",
                "\n",
                "app_code = '''\n",
                "import gradio as gr\n",
                "import numpy as np\n",
                "import joblib\n",
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "\n",
                "# Load model and preprocessors\n",
                "model = keras.models.load_model('model.h5')\n",
                "scaler = joblib.load('scaler.pkl')\n",
                "feature_names = joblib.load('feature_names.pkl')\n",
                "\n",
                "def predict_sales(*inputs):\n",
                "    try:\n",
                "        input_data = np.array([list(inputs)])\n",
                "        input_scaled = scaler.transform(input_data)\n",
                "        prediction = model.predict(input_scaled, verbose=0)[0][0]\n",
                "        return f\"üí∞ **Predicted Weekly Sales: ${prediction:,.2f}**\"\n",
                "    except Exception as e:\n",
                "        return f\"‚ùå Error: {str(e)}\"\n",
                "\n",
                "# Create inputs\n",
                "inputs = [gr.Number(label=feature, value=0) for feature in feature_names]\n",
                "\n",
                "# Create interface\n",
                "demo = gr.Interface(\n",
                "    fn=predict_sales,\n",
                "    inputs=inputs,\n",
                "    outputs=gr.Textbox(label=\"Prediction\", lines=2),\n",
                "    title=\"üè™ Walmart Weekly Sales Predictor\",\n",
                "    description=\"Simple Neural Network Regression Model\",\n",
                "    theme=gr.themes.Soft()\n",
                ")\n",
                "\n",
                "demo.launch()\n",
                "'''\n",
                "\n",
                "print(\"Copy the code above to create app.py in your Hugging Face Space\")\n",
                "print(\"\\nOr download it:\")\n",
                "\n",
                "with open('app.py', 'w') as f:\n",
                "    f.write(app_code)\n",
                "    \n",
                "files.download('app.py')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìã requirements.txt Code"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "requirements = '''tensorflow==2.13.0\n",
                "gradio==4.19.2\n",
                "joblib==1.3.2\n",
                "numpy==1.24.3\n",
                "'''\n",
                "\n",
                "with open('requirements.txt', 'w') as f:\n",
                "    f.write(requirements)\n",
                "    \n",
                "print(\"requirements.txt created!\")\n",
                "files.download('requirements.txt')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## ‚úÖ PROJECT COMPLETE!\n",
                "\n",
                "### What You Have:\n",
                "- ‚úÖ Trained Neural Network model\n",
                "- ‚úÖ Model evaluation metrics\n",
                "- ‚úÖ Training visualizations\n",
                "- ‚úÖ Saved model files\n",
                "- ‚úÖ Deployment code (app.py)\n",
                "- ‚úÖ Requirements file\n",
                "\n",
                "### Next Steps:\n",
                "1. Download all files from this notebook\n",
                "2. Create Hugging Face Space\n",
                "3. Upload files to Space\n",
                "4. Deploy and share!\n",
                "\n",
                "### For Local Deployment:\n",
                "1. Save all files to a local folder\n",
                "2. Install requirements: `pip install -r requirements.txt`\n",
                "3. Run: `python app.py`\n",
                "4. Open: `http://localhost:7860`\n",
                "\n",
                "---\n",
                "\n",
                "**Made with ‚ù§Ô∏è for academic submission**\n",
                "\n",
                "**Date:** 2026-02-13"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}